{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGm4fad3M-Sr"
      },
      "source": [
        "#  DVS Object Detection Benchmark Tutorial\n",
        "\n",
        "This tutorial aims to provide an insight on how the NeuroBench framework is organized and how you can use it to benchmark your own models!\n",
        "\n",
        "## About DVS Object Detection:\n",
        "Real-time object detection is a widely used computer vision task with applications in several domains, including robotics, autonomous driving, and surveillance. Its applications include event cameras for smart home and surveillance systems, drones that monitor and track objects of interest, and self-driving cars that detect obstacles to ensure safe operation. Efficient energy consumption and real-time performance are crucial in such scenarios, particularly when deployed on low-power or always-on edge devices.\n",
        "\n",
        "### Dataset:\n",
        "The object detection benchmark utilizes the Prophesee 1 Megapixel Automotive Detection Dataset. This dataset was recorded with a high-resolution event camera with a 110 degree field of view mounted on a car windshield. The car was driven in various areas under different daytime weather conditions over several months. The dataset was labeled using the video stream of an additional RGB camera in a semi-automated way, resulting in over 25 million bounding boxes for seven different object classes: pedestrian, two-wheeler, car, truck, bus, traffic sign, and traffic light. The labels are provided at a rate of 60Hz, and the recording of 14.65 hours is split into 11.19, 2.21, and 2.25 hours for training, validation, and testing, respectively. \n",
        "\n",
        "### Benchmark Task:\n",
        "The task of object detection in event-based spatio-temporal data involves identifying bounding boxes of objects belonging to multiple predetermined classes in an event stream. Training for this task is performed offline based on the data splits provided by the original dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we will import the relevant libraries. These include the datasets, preprocessors and postprocessors. To ensure your model to be compatible with the NeuroBench framework, we will import the wrapper for snnTorch models. This wrapper will not change your model. Finally, we import the Benchmark class, which will run the benchmark and calculate your metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from neurobench.datasets import Gen4DetectionDataLoader\n",
        "from neurobench.models import NeuroBenchModel\n",
        "from neurobench.benchmarks import Benchmark\n",
        "\n",
        "from metavision_ml.detection.anchors import Anchors\n",
        "from metavision_ml.detection.rpn import BoxHead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this tutorial, we will make use of the example architecture that is included in the NeuroBench framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from obj_det_model import Vanilla, Vanilla_lif\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get started, we will load our desired dataset in a dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dataloader itself takes about 7 minutes for loading, with model evaluation and score calculation is about 20 minutes on i9-12900KF, RTX3080\n",
        "test_set_dataloader = Gen4DetectionDataLoader(dataset_path=\"data/Gen 4 Multi channel\",\n",
        "        split=\"testing\",\n",
        "        label_map_path=\"neurobench/datasets/label_map_dictionary.json\",\n",
        "        batch_size = 12,\n",
        "        num_tbins = 12,\n",
        "        preprocess_function_name=\"multi_channel_timesurface\",\n",
        "        delta_t=50000,\n",
        "        channels=6,  # multichannel six channels\n",
        "        height=360,\n",
        "        width=640,\n",
        "        max_incr_per_pixel=5,\n",
        "        class_selection=[\"pedestrian\", \"two wheeler\", \"car\"],\n",
        "        num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the models we want to benchmark, we need a wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation pipeline written and models trained by Shenqi Wang (wang69@imec.be) and Guangzhi Tang (guangzhi.tang@imec.nl) at imec.\n",
        "\n",
        "class ObjDetectionModel(NeuroBenchModel):\n",
        "    def __init__(self, net, box_coder, head):\n",
        "        self.net = net\n",
        "        self.box_coder = box_coder\n",
        "        self.head = head\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        self.net.eval()\n",
        "        inputs = batch.permute(1, 0, 2, 3, 4).to(device='cuda') # dataloader supplies batch,timestep,*; model expects timestep,batch,*\n",
        "        with torch.no_grad():\n",
        "            feature = self.net(inputs)\n",
        "            loc_preds_val, cls_preds_val = self.head(feature)\n",
        "            scores = self.head.get_scores(cls_preds_val)\n",
        "            scores = scores.to('cpu')\n",
        "            for i, feat in enumerate(feature):\n",
        "                feature[i] = feature[i].to('cpu')\n",
        "            inputs = inputs.to('cpu')\n",
        "            loc_preds_val = loc_preds_val.to('cpu')\n",
        "            preds = box_coder.decode(feature, inputs, loc_preds_val, scores, batch_size=inputs.shape[1], score_thresh=0.05,\n",
        "                        nms_thresh=0.5, max_boxes_per_input=500)\n",
        "        return preds\n",
        "\n",
        "    def __net__(self):\n",
        "        # returns only network, not box_coder and head\n",
        "        return self.net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we load our model. This example includes two possibilities, a hybrid model which uses artificial neurons and spiking neurons or a fully artificial neural network without spiking neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the model\n",
        "mode = \"hybrid\" # \"ann\" or \"hybrid\n",
        "if mode == \"ann\":\n",
        "    # baseline ANN RED architecture\n",
        "    model = Vanilla(cin = 6, cout = 256, base = 16)\n",
        "    box_coder = Anchors(num_levels=model.levels, anchor_list=\"PSEE_ANCHORS\", variances=[0.1, 0.2])\n",
        "    head = BoxHead(model.cout, box_coder.num_anchors, 3+1, 0)\n",
        "    model = model.to('cuda')\n",
        "    head = head.to('cuda')\n",
        "    model.load_state_dict(torch.load('neurobench/examples/obj_detection/model_data/save_models/25_ann_model.pth',map_location=torch.device('cuda')))\n",
        "    head.load_state_dict(torch.load('neurobench/examples/obj_detection/model_data/save_models/25_ann_pd.pth',map_location=torch.device('cuda')))\n",
        "elif mode == \"hybrid\":\n",
        "    # hybrid SNN of above architecture\n",
        "    model = Vanilla_lif(cin = 6, cout = 256, base = 16)\n",
        "    box_coder = Anchors(num_levels=model.levels, anchor_list=\"PSEE_ANCHORS\", variances=[0.1, 0.2])\n",
        "    head = BoxHead(model.cout, box_coder.num_anchors, 3+1, 0)\n",
        "    model = model.to('cuda')\n",
        "    head = head.to('cuda')\n",
        "    model.load_state_dict(torch.load('neurobench/examples/obj_detection/model_data/save_models/14_hybrid_model.pth',map_location=torch.device('cuda')))\n",
        "    head.load_state_dict(torch.load('neurobench/examples/obj_detection/model_data/save_models/14_hybrid_pd.pth',map_location=torch.device('cuda')))\n",
        "else:\n",
        "    raise ValueError(\"mode must be ann or hybrid\")\n",
        "\n",
        "model = ObjDetectionModel(model, box_coder, head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we load the preprocessors and postprocessors we would like to apply."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessors = []\n",
        "postprocessors = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next specify the metrics which you want to calculate. The available metrics (V1.0 release) include:\n",
        "\n",
        "static_metrics:\n",
        "*   model_size\n",
        "*   connection_sparsity\n",
        "*   parameter_count\n",
        "*   Model Excecution Rate\n",
        "\n",
        "\n",
        "workload_metrics\n",
        "*   activation_sparsity\n",
        "*   synaptic_operations\n",
        "*   classification_accuracy\n",
        "*   coco_map\n",
        "*   mse\n",
        "*   r2\n",
        "*   smape\n",
        " \n",
        "For more information about the metrics, you can consult the documentation. Note that the Model Excecution Rate is not returned by the famework, but reported by the user. Execution rate, in Hz, of the model computation based on forward inference passes per second, measured in the time-stepped simulation timescale. More explanation on the metrics can be found on neurobench.ai <https://neurobench.ai/>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "static_metrics = [\"model_size\", \"connection_sparsity\"]\n",
        "workload_metrics = [\"COCO_mAP\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you are ready to run the benchmark!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "benchmark = Benchmark(model, test_set_dataloader, preprocessors, postprocessors, [static_metrics, workload_metrics])\n",
        "results = benchmark.run()\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
