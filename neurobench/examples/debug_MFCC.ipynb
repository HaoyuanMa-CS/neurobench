{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader, ConcatDataset, TensorDataset\n",
    "import torchaudio\n",
    "\n",
    "from torch import nn, optim, distributions as dist\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import learn2learn as l2l\n",
    "import copy\n",
    "\n",
    "# from torch_mate.data.utils import IncrementalFewShot\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home3/p306982/Simulations/fscil/algorithms_benchmarks/\")\n",
    "\n",
    "from neurobench.datasets import MSWC\n",
    "from neurobench.preprocessing.speech2spikes import S2SProcessor\n",
    "from neurobench.datasets.IncrementalFewShot import IncrementalFewShot\n",
    "from neurobench.examples.model_data.M5 import M5\n",
    "\n",
    "from neurobench.benchmarks import Benchmark\n",
    "from cl_utils import *\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"//scratch/p306982/data/fscil/mswc/\"\n",
    "NUM_WORKERS = 8\n",
    "BATCH_SIZE = 256\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"training\", incremental=True)\n",
    "pre_train_loader = DataLoader(base_train_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
    "base_test_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\", incremental=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00, -3.0518e-05,  ..., -3.0518e-05,\n",
       "           0.0000e+00,  0.0000e+00]]),\n",
       " 48,\n",
       " '//scratch/p306982/data/fscil/mswc/en/clips',\n",
       " 'add/common_voice_en_100260.opus')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0000e+00,  0.0000e+00, -3.0518e-05,  ..., -3.0518e-05,\n",
       "           0.0000e+00,  0.0000e+00]]),\n",
       " 48,\n",
       " '//scratch/p306982/data/fscil/mswc/en/clips',\n",
       " 'add/common_voice_en_100260.opus')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_test_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'see': 11526,\n",
       "  'think': 9085,\n",
       "  'work': 7944,\n",
       "  'find': 6633,\n",
       "  'home': 5877,\n",
       "  'look': 5137,\n",
       "  'music': 4657,\n",
       "  'next': 4576,\n",
       "  'heart': 4317,\n",
       "  'play': 3850,\n",
       "  'read': 3597,\n",
       "  'show': 3574,\n",
       "  'game': 3569,\n",
       "  'set': 3291,\n",
       "  'open': 3042,\n",
       "  'sound': 2769,\n",
       "  'story': 2702,\n",
       "  'understand': 2561,\n",
       "  'talk': 2503,\n",
       "  'keep': 2466,\n",
       "  'call': 2430,\n",
       "  'stop': 2398,\n",
       "  'change': 2172,\n",
       "  'hear': 2161,\n",
       "  'run': 2139,\n",
       "  'start': 2132,\n",
       "  'feel': 2115,\n",
       "  'remember': 2079,\n",
       "  'speak': 1978,\n",
       "  'eat': 1944,\n",
       "  'present': 1923,\n",
       "  'center': 1900,\n",
       "  'phone': 1845,\n",
       "  'order': 1820,\n",
       "  'learn': 1818,\n",
       "  'close': 1809,\n",
       "  'ask': 1782,\n",
       "  'style': 1732,\n",
       "  'season': 1720,\n",
       "  'news': 1709,\n",
       "  'research': 1660,\n",
       "  'turn': 1659,\n",
       "  'write': 1592,\n",
       "  'drink': 1574,\n",
       "  'search': 1567,\n",
       "  'market': 1561,\n",
       "  'track': 1493,\n",
       "  'project': 1489,\n",
       "  'add': 1482,\n",
       "  'watch': 1474,\n",
       "  'forget': 1464,\n",
       "  'position': 1432,\n",
       "  'listen': 1406,\n",
       "  'sort': 1394,\n",
       "  'design': 1334,\n",
       "  'bank': 1291,\n",
       "  'sit': 1271,\n",
       "  'video': 1248,\n",
       "  'save': 1241,\n",
       "  'deal': 1233,\n",
       "  'lights': 1229,\n",
       "  'meet': 1225,\n",
       "  'walk': 1212,\n",
       "  'cut': 1183,\n",
       "  'question': 1167,\n",
       "  'weather': 1158,\n",
       "  'date': 1131,\n",
       "  'capital': 1130,\n",
       "  'record': 1111,\n",
       "  'ready': 1100,\n",
       "  'store': 1073,\n",
       "  'stand': 1063,\n",
       "  'test': 1062,\n",
       "  'build': 1045,\n",
       "  'forward': 1034,\n",
       "  'previous': 1019,\n",
       "  'cover': 995,\n",
       "  'coffee': 963,\n",
       "  'explain': 953,\n",
       "  'plan': 952,\n",
       "  'blow': 940,\n",
       "  'hit': 937,\n",
       "  'thank': 932,\n",
       "  'picture': 928,\n",
       "  'dust': 920,\n",
       "  'dance': 917,\n",
       "  'sleep': 913,\n",
       "  'fact': 896,\n",
       "  'color': 888,\n",
       "  'place': 881,\n",
       "  'report': 878,\n",
       "  'teach': 865,\n",
       "  'dry': 835,\n",
       "  'restaurant': 811,\n",
       "  'clean': 808,\n",
       "  'trail': 791,\n",
       "  'complete': 788,\n",
       "  'text': 785,\n",
       "  'study': 785,\n",
       "  'imagine': 760},\n",
       " {'send': 711,\n",
       "  'create': 699,\n",
       "  'review': 681,\n",
       "  'jump': 673,\n",
       "  'perform': 662,\n",
       "  'allow': 659,\n",
       "  'pull': 643,\n",
       "  'rate': 640,\n",
       "  'throw': 635,\n",
       "  'flight': 623,\n",
       "  'ride': 619,\n",
       "  'reach': 619,\n",
       "  'temperature': 613,\n",
       "  'direct': 613,\n",
       "  'price': 606,\n",
       "  'finish': 602,\n",
       "  'catch': 583,\n",
       "  'smell': 573,\n",
       "  'cry': 564,\n",
       "  'schedule': 558,\n",
       "  'channel': 527,\n",
       "  'drop': 527,\n",
       "  'package': 524,\n",
       "  'label': 520,\n",
       "  'observe': 520,\n",
       "  'status': 503,\n",
       "  'condition': 501,\n",
       "  'count': 492,\n",
       "  'taste': 478,\n",
       "  'clock': 476,\n",
       "  'pack': 471,\n",
       "  'object': 462,\n",
       "  'share': 461,\n",
       "  'replace': 452,\n",
       "  'exchange': 445,\n",
       "  'laugh': 442,\n",
       "  'correct': 437,\n",
       "  'paint': 435,\n",
       "  'accept': 432,\n",
       "  'notice': 428,\n",
       "  'stick': 427,\n",
       "  'prepare': 427,\n",
       "  'float': 426,\n",
       "  'polish': 422,\n",
       "  'request': 415,\n",
       "  'reply': 410,\n",
       "  'guide': 407,\n",
       "  'stock': 405,\n",
       "  'smile': 404,\n",
       "  'receive': 403,\n",
       "  'apply': 401,\n",
       "  'interpret': 396,\n",
       "  'sing': 393,\n",
       "  'wake': 381,\n",
       "  'log': 370,\n",
       "  'format': 364,\n",
       "  'message': 362,\n",
       "  'cleaning': 358,\n",
       "  'forecast': 354,\n",
       "  'steam': 347,\n",
       "  'chart': 346,\n",
       "  'describe': 339,\n",
       "  'touch': 334,\n",
       "  'express': 333,\n",
       "  'appeal': 332,\n",
       "  'kick': 325,\n",
       "  'hang': 324,\n",
       "  'volume': 323,\n",
       "  'measure': 322,\n",
       "  'map': 321,\n",
       "  'ignore': 320,\n",
       "  'copy': 319,\n",
       "  'link': 319,\n",
       "  'develop': 318,\n",
       "  'kiss': 316,\n",
       "  'discuss': 308,\n",
       "  'lift': 298,\n",
       "  'treat': 296,\n",
       "  'resume': 292,\n",
       "  'rotate': 291,\n",
       "  'taxi': 290,\n",
       "  'protect': 288,\n",
       "  'capture': 288,\n",
       "  'grant': 288,\n",
       "  'challenge': 288,\n",
       "  'print': 281,\n",
       "  'swim': 279,\n",
       "  'slide': 279,\n",
       "  'shake': 272,\n",
       "  'pardon': 270,\n",
       "  'contrast': 264,\n",
       "  'alarm': 262,\n",
       "  'tape': 258,\n",
       "  'recommend': 258,\n",
       "  'photograph': 256,\n",
       "  'draw': 253,\n",
       "  'pat': 250,\n",
       "  'celebrate': 250,\n",
       "  'seek': 250,\n",
       "  'contest': 243})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from neurobench.datasets.MSWC import generate_mswc_fscil_splits\n",
    "generate_mswc_fscil_splits(ROOT, [\"en\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2S = S2SProcessor(device)\n",
    "S2S._default_spec_kwargs[\"sample_rate\"] = 48000\n",
    "pre_proc = S2S.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home3/p306982/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/functional/functional.py:576: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (256) may be set too high. Or, the value for `n_freqs` (1025) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio.transforms as T\n",
    "\n",
    "n_fft = 2048\n",
    "win_length = None\n",
    "hop_length = 240\n",
    "n_mels = 256\n",
    "n_mfcc = 256\n",
    "\n",
    "mfcc_transform = T.MFCC(\n",
    "    sample_rate=48000,\n",
    "    n_mfcc=n_mfcc,\n",
    "    melkwargs={\n",
    "        \"n_fft\": n_fft,\n",
    "        \"n_mels\": n_mels,\n",
    "        \"hop_length\": hop_length,\n",
    "        \"mel_scale\": \"htk\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 201])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = base_train_set[0][0]\n",
    "mfcc = mfcc_transform(data)\n",
    "mfcc.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3.2433e-09, 2.1147e-09, 7.0820e-10,  ..., 4.3519e-09,\n",
       "          2.3478e-09, 5.4743e-10],\n",
       "         [7.2043e-10, 3.6058e-10, 9.0458e-11,  ..., 4.3140e-09,\n",
       "          3.5827e-09, 1.4136e-09],\n",
       "         [1.2516e-09, 3.6034e-10, 4.4313e-10,  ..., 3.1022e-09,\n",
       "          2.3836e-09, 6.7672e-10],\n",
       "         ...,\n",
       "         [1.5025e-09, 8.5147e-10, 5.5862e-10,  ..., 6.2881e-10,\n",
       "          1.3803e-10, 4.6415e-10],\n",
       "         [3.2382e-10, 1.4809e-09, 1.3067e-09,  ..., 3.3691e-10,\n",
       "          2.7184e-10, 2.6055e-10],\n",
       "         [3.2041e-09, 1.3078e-09, 1.9749e-09,  ..., 4.5405e-10,\n",
       "          1.9087e-09, 9.1985e-10]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mfcc==0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 133/195 [00:46<00:21,  2.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m zero_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m,))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(pre_train_loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(base_train_set)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mBATCH_SIZE):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     mfcc \u001b[39m=\u001b[39m mfcc_transform(data)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Ba100gpu2/home3/p306982/Simulations/fscil/algorithms_benchmarks/neurobench/examples/debug_MFCC.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     zero_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39msum(mfcc\u001b[39m==\u001b[39m\u001b[39m0.0\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:730\u001b[0m, in \u001b[0;36mMFCC.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    723\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    724\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[39m        Tensor: specgram_mel_db of size (..., ``n_mfcc``, time).\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 730\u001b[0m     mel_specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMelSpectrogram(waveform)\n\u001b[1;32m    731\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_mels:\n\u001b[1;32m    732\u001b[0m         log_offset \u001b[39m=\u001b[39m \u001b[39m1e-6\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:650\u001b[0m, in \u001b[0;36mMelSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    643\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    644\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39m        Tensor: Mel frequency spectrogram of size (..., ``n_mels``, time).\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 650\u001b[0m     specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspectrogram(waveform)\n\u001b[1;32m    651\u001b[0m     mel_specgram \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmel_scale(specgram)\n\u001b[1;32m    652\u001b[0m     \u001b[39mreturn\u001b[39;00m mel_specgram\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/transforms/_transforms.py:110\u001b[0m, in \u001b[0;36mSpectrogram.forward\u001b[0;34m(self, waveform)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, waveform: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    101\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[39m        waveform (Tensor): Tensor of audio of dimension (..., time).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m        Fourier bins, and time is the number of window hops (n_frame).\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mspectrogram(\n\u001b[1;32m    111\u001b[0m         waveform,\n\u001b[1;32m    112\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad,\n\u001b[1;32m    113\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwindow,\n\u001b[1;32m    114\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_fft,\n\u001b[1;32m    115\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhop_length,\n\u001b[1;32m    116\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwin_length,\n\u001b[1;32m    117\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpower,\n\u001b[1;32m    118\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnormalized,\n\u001b[1;32m    119\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcenter,\n\u001b[1;32m    120\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_mode,\n\u001b[1;32m    121\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49monesided,\n\u001b[1;32m    122\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/pytorch_cuda/lib/python3.9/site-packages/torchaudio/functional/functional.py:147\u001b[0m, in \u001b[0;36mspectrogram\u001b[0;34m(waveform, pad, window, n_fft, hop_length, win_length, power, normalized, center, pad_mode, onesided, return_complex)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m power \u001b[39m==\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m    146\u001b[0m         \u001b[39mreturn\u001b[39;00m spec_f\u001b[39m.\u001b[39mabs()\n\u001b[0;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m spec_f\u001b[39m.\u001b[39;49mabs()\u001b[39m.\u001b[39mpow(power)\n\u001b[1;32m    148\u001b[0m \u001b[39mreturn\u001b[39;00m spec_f\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "zero_count = torch.zeros((1,))\n",
    "for batch_idx, (data, target) in tqdm(enumerate(pre_train_loader), total=len(base_train_set)//BATCH_SIZE):\n",
    "    mfcc = mfcc_transform(data)\n",
    "    zero_count +=torch.sum(mfcc==0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.isinf(mfcc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5_Feature(nn.Module):\n",
    "    def __init__(self, n_input=1, stride=16, n_channel=32):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=8, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.pool4 = nn.MaxPool1d(2)\n",
    "        # self.avg_pool = nn.AvgPool1d(64)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.act3(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.act4(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        # x = self.avg_pool(x)\n",
    "        # x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M5_Feature(256, 1, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = model(mfcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9530, 0.0000, 0.0000,  ..., 0.4128, 0.0000, 0.0000],\n",
       "         [0.3260, 0.5191, 0.7440,  ..., 0.5860, 1.0364, 0.8143],\n",
       "         [0.2114, 0.6727, 0.6269,  ..., 1.1413, 0.7937, 0.9362],\n",
       "         ...,\n",
       "         [1.9893, 0.6086, 0.4245,  ..., 0.4697, 0.4444, 0.0318],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0509, 0.5623,  ..., 0.6040, 0.6049, 1.2391]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 10])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pool = nn.AvgPool1d(10)\n",
    "feature = avg_pool(data)\n",
    "feature.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
