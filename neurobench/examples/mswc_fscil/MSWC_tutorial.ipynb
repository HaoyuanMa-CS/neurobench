{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGm4fad3M-Sr"
      },
      "source": [
        "# MSWC FSCIL Benchmark Tutorial\n",
        "\n",
        "This tutorial aims to provide an insight on how the NeuroBench framework is organized and how you can use it to benchmark your own models!\n",
        "\n",
        "## About MSCW FSCIL (Keyword Few-Shot Class-Incremental Learning)\n",
        "\n",
        "Learning new tasks from a small amount of experiences while retaining knowledge of prior tasks is a hallmark of biological intelligence and a long-standing goal of general AI. It is especially a key challenge to endow edge devices with the ability to adapt to their environments and users. This benchmark thus evaluates the capacity of a model to successively incorporate new keywords over multiple sessions (class-incremental), with only a handful of samples from the new classes to train with (few-shot). The FSCIL task is a recently established benchmark in the computer vision domain, but it has not yet been adapted to other data modalities. \n",
        "\n",
        "### Dataset:\n",
        "Aligning with a neuromorphic interest in temporal data modalities, this benchmark introduces a FSCIL task with streaming audio data using the large Multilingual Spoken Word Corpus (MSWC) keyword classification dataset. The task is designed to be approached in two phases: pre-training and incremental learning. First, for pre-training, a set of 100 words spanning 5 base languages (English, German, Catalan, French, Kinyarwanda) with 500 training samples each are made available to train an initial model. Next, for incremental learning, the model undergoes 10 successive sessions to learn words from 10 new languages (Persian, Spanish, Russian, Welsh, Italian, Basque, Polish, Esparanto, Portuguese, Dutch) in a few-shot learning scenario. Each incremental session adds 10 words of the corresponding session language with only 5 training samples available per word. After each session, the model is tested in classification accuracy on all prior learned classes, including the 100 base pre-training classes and the few-shot-learned classes, therefore evaluating the FSCIL solution on its ability to learn new classes while retaining knowledge about the previously learned ones. Each session learns a new language, for a total knowledge base of 200 keywords by the end of the benchmark.\n",
        "\n",
        "\n",
        "### Benchmark Task:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, load your model that is pre-trained on the MSWC base training subset (in code: `MSWC(root=..., subset=\"base\", procedure=\"training\")`). Note that is should not have a classification layer at the end, as this will be added by the benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, convert it to a NeuroBench TorchModel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neurobench.models import TorchModel\n",
        "\n",
        "model = TorchModel(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redefine the following constants as per your liking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT = \"./FSCIL_subset/\" # Where the MSWC dataset is stored\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 256\n",
        "NUM_REPEATS = 5 # How many times to repeat the experiment to get aggregate statistics\n",
        "SPIKING = False\n",
        "EVAL_SHOTS = 5 # How many shots to use for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the modules required for running the benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "from neurobench.benchmarks import Benchmark\n",
        "from neurobench.datasets import MSWC\n",
        "from neurobench.datasets.MSWC_IncrementalLoader import IncrementalFewShot\n",
        "\n",
        "from mswc_fscil_proto import to_device, squeeze, examples_per_class, out2pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select the desired device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if device == torch.device(\"cuda\"):\n",
        "    PIN_MEMORY = True\n",
        "else:\n",
        "    PIN_MEMORY = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the MFCC pre-processing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neurobench.preprocessing import MFCCProcessor, S2SProcessor\n",
        "\n",
        "n_fft = 512\n",
        "win_length = None\n",
        "hop_length = 240\n",
        "n_mels = 20\n",
        "n_mfcc = 20\n",
        "\n",
        "if SPIKING:\n",
        "    encode = S2SProcessor(device, transpose=False)\n",
        "    config_change = {\"sample_rate\": 48000,\n",
        "                     \"hop_length\": 240}\n",
        "    encode.configure(threshold=1.0, **config_change)\n",
        "else:\n",
        "    encode = MFCCProcessor(\n",
        "        sample_rate=48000,\n",
        "        n_mfcc=n_mfcc,\n",
        "        melkwargs={\n",
        "            \"n_fft\": n_fft,\n",
        "            \"n_mels\": n_mels,\n",
        "            \"hop_length\": hop_length,\n",
        "            \"mel_scale\": \"htk\",\n",
        "            \"f_min\": 20,\n",
        "            \"f_max\": 4000,\n",
        "        },\n",
        "        device = device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the base training dataset to generate the prototypical representations for the base classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"training\")\n",
        "train_loader = DataLoader(base_train_set, batch_size=500, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the prototypical representations for the base classes and store them in a new linear layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "if SPIKING:\n",
        "    output = model.net.snn[-1].W\n",
        "    proto_out = nn.Linear(output.weight.shape[1], 200, bias=True).to(device)\n",
        "    proto_out.weight.data = output.weight.data\n",
        "else:\n",
        "    output = model.net.output\n",
        "    proto_out = nn.Linear(512, 200, bias=True).to(device)\n",
        "    proto_out.weight.data = output.weight.data\n",
        "\n",
        "for data, target in train_loader:\n",
        "    data, target = encode((data.to(device), target.to(device)))\n",
        "    data = data.squeeze()\n",
        "    class_id = target[0]\n",
        "\n",
        "    if SPIKING:\n",
        "        features = data\n",
        "        for layer in model.net.snn[:-1]:\n",
        "            features = layer(features)\n",
        "\n",
        "        mean = torch.sum(features, dim=[0,1])/500\n",
        "        proto_out.weight.data[class_id] = 2*mean\n",
        "        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())/features.shape[1]\n",
        "\n",
        "    else:\n",
        "        features = model.net(data, features_out=True)\n",
        "\n",
        "        mean = torch.sum(features, dim=0)/500\n",
        "        proto_out.weight.data[class_id] = 2*mean\n",
        "        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())\n",
        "\n",
        "    del data\n",
        "    del features\n",
        "    del mean\n",
        "\n",
        "if SPIKING:\n",
        "    model.net.snn[-1].W = proto_out\n",
        "else:\n",
        "    model.net.output = proto_out\n",
        "\n",
        "del base_train_set\n",
        "del train_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, test the performance of the prototypical representations on the base test set using a Neurobench Benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Evaluation phase ###\n",
        "\n",
        "eval_model = copy.deepcopy(model)\n",
        "\n",
        "eval_accs = []\n",
        "query_accs = []\n",
        "act_sparsity = []\n",
        "syn_ops_dense = []\n",
        "syn_ops_macs = []\n",
        "syn_ops_acs = []\n",
        "\n",
        "# Get base test set for evaluation\n",
        "base_test_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\")\n",
        "test_loader = DataLoader(base_test_set, batch_size=256, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "# Define an arbitrary resampling as an example of pre-processor to feed to the Benchmark object\n",
        "eval_model.net.eval()\n",
        "\n",
        "# Metrics\n",
        "static_metrics = [\"model_size\", \"connection_sparsity\"]\n",
        "workload_metrics = [\"classification_accuracy\", \"activation_sparsity\", \"synaptic_operations\"]\n",
        "\n",
        "# Define benchmark object\n",
        "benchmark_all_test = Benchmark(eval_model, metric_list=[static_metrics, workload_metrics], dataloader=test_loader, \n",
        "                    preprocessors=[to_device, encode, squeeze], postprocessors=[])\n",
        "\n",
        "benchmark_new_classes = Benchmark(eval_model, metric_list=[[],[\"classification_accuracy\"]], dataloader=test_loader,\n",
        "                    preprocessors=[to_device, encode, squeeze], postprocessors=[])\n",
        "\n",
        "# Define specific post-processing with masking on the base classes\n",
        "mask = torch.full((200,), float('inf')).to(device)\n",
        "mask[torch.arange(0,100, dtype=int)] = 0\n",
        "out_mask = lambda x: x - mask\n",
        "\n",
        "# Run session 0 benchmark on base classes\n",
        "print(f\"Session: 0\")\n",
        "\n",
        "pre_train_results = benchmark_all_test.run(postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "\n",
        "print(\"Base results:\", pre_train_results)\n",
        "\n",
        "eval_accs.append(pre_train_results['classification_accuracy'])\n",
        "act_sparsity.append(pre_train_results['activation_sparsity'])\n",
        "syn_ops_dense.append(pre_train_results['synaptic_operations']['Dense'])\n",
        "syn_ops_macs.append(pre_train_results['synaptic_operations']['Effective_MACs'])\n",
        "syn_ops_acs.append(pre_train_results['synaptic_operations']['Effective_ACs'])\n",
        "\n",
        "print(f\"The base accuracy is {eval_accs[-1]*100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The accuracy on the base classes using prototypical representations should be a bit lower than the accuracy of the original model. This is due to the conversion from the original backpropagation-trained readout classifier to the prototype readout classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, it is possible can run the incremental learning benchmark! Note however, that the code below supports both spiking and non-spiking networks (hence the flag `SPIKING` in an earlier cell). For more details on how the spiking network is constructed, please refer to [`mswc_fscil_proto.py`](./mswc_fscil_proto.py). Note that in the code below, only 'one' repeat is performed whereas in the referenced Python file, `n` repeats are executed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IncrementalFewShot Dataloader used in incremental mode to generate class-incremental sessions\n",
        "few_shot_dataloader = IncrementalFewShot(n_way=10, k_shot=EVAL_SHOTS, \n",
        "                            root = ROOT,\n",
        "                            query_shots=100,\n",
        "                            support_query_split=(100,100),\n",
        "                            samples_per_class=200)\n",
        "\n",
        "# Iteration over incremental sessions\n",
        "for session, (support, query, query_classes) in enumerate(few_shot_dataloader):\n",
        "    print(f\"Session: {session+1}\")\n",
        "\n",
        "    # Define benchmark object\n",
        "    benchmark_all_test = Benchmark(eval_model, metric_list=[static_metrics, workload_metrics], dataloader=test_loader, \n",
        "                        preprocessors=[to_device, encode, squeeze], postprocessors=[])\n",
        "\n",
        "    benchmark_new_classes = Benchmark(eval_model, metric_list=[[],[\"classification_accuracy\"]], dataloader=test_loader,\n",
        "                        preprocessors=[to_device, encode, squeeze], postprocessors=[])\n",
        "    \n",
        "    cur_class = support[0][1].tolist()\n",
        "    eval_model.net.cur_j = examples_per_class(cur_class, 200, 5)\n",
        "\n",
        "    ### Computing new Prototypical Weights ###\n",
        "    data = None\n",
        "    \n",
        "    for X_shot, y_shot in support:\n",
        "        if data is None:\n",
        "            data = X_shot\n",
        "            target = y_shot\n",
        "        else:\n",
        "            data = torch.cat((data,X_shot), 0)\n",
        "            target = torch.cat((target,y_shot), 0)\n",
        "\n",
        "    data, target = encode((data.to(device), target.to(device)))\n",
        "    data = data.squeeze()\n",
        "\n",
        "    if SPIKING:\n",
        "        features = eval_model.net.snn[0](data)\n",
        "        features = eval_model.net.snn[1](features)\n",
        "\n",
        "        for index, class_id  in enumerate(query_classes[-10:]):\n",
        "            mean = torch.sum(features[[i*10+index for i in range(EVAL_SHOTS)]], dim=[0,1])/EVAL_SHOTS\n",
        "            eval_model.net.snn[-1].W.weight.data[class_id] = 2*mean\n",
        "            eval_model.net.snn[-1].W.bias.data[class_id] = -torch.matmul(mean, mean.t())/(features.shape[1])\n",
        "    else:\n",
        "        features = eval_model.net(data, features_out=True)\n",
        "\n",
        "        for index, class_id  in enumerate(query_classes[-10:]):\n",
        "            mean = torch.sum(features[[i*10+index for i in range(EVAL_SHOTS)]], dim=0)/EVAL_SHOTS\n",
        "            eval_model.net.output.weight.data[class_id] = 2*mean\n",
        "            eval_model.net.output.bias.data[class_id] = -torch.matmul(mean, mean.t())\n",
        "\n",
        "    ### Testing phase ###\n",
        "    eval_model.net.eval()\n",
        "\n",
        "    # Define session dataloaders for query and query + base_test samples\n",
        "    query_loader = DataLoader(query, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "    \n",
        "    full_session_test_set = ConcatDataset([base_test_set, query])\n",
        "    full_session_test_loader = DataLoader(full_session_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # Create a mask function to only consider accuracy on classes presented so far\n",
        "    session_classes = torch.cat((torch.arange(0,100, dtype=int), torch.IntTensor(query_classes))) \n",
        "    mask = torch.full((200,), float('inf')).to(device)\n",
        "    mask[session_classes] = 0\n",
        "    out_mask = lambda x: x - mask\n",
        "\n",
        "    # Run benchmark to evaluate accuracy of this specific session\n",
        "    session_results = benchmark_all_test.run(dataloader = full_session_test_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "    print(\"Session results:\", session_results)\n",
        "    \n",
        "    # Run benchmark on query classes only\n",
        "    query_results = benchmark_new_classes.run(dataloader = query_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "    print(f\"Accuracy on new classes: {query_results['classification_accuracy']*100} %\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
