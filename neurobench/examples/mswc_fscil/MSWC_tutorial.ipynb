{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGm4fad3M-Sr"
      },
      "source": [
        "# MSWC FSCIL Neurobench Tutorial\n",
        "\n",
        "This tutorial aims to provide an insight on the MSWC FSCIL NeuroBench task and present how you can use the corresponding Neurobench harness to benchmark your own models and solutions! In particular we give a tutorial to implement the prototypical network approach to both a convolutional and a recurrent spiking network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction:\n",
        "\n",
        "### About FSCIL (Few-Shot Class-Incremental Learning)\n",
        "\n",
        "Learning new tasks from a small amount of experiences while retaining knowledge of prior tasks is a hallmark of biological intelligence and a long-standing goal of general AI. It is especially a key challenge to endow edge devices with the ability to adapt to their environments and users. This benchmark thus evaluates the capacity of a learning solution to successively incorporate new classes over multiple sessions (class-incremental), with only a handful of samples from the new classes to train with (few-shot). The FSCIL task is a recently established benchmark in the computer vision domain, but it has not yet been adapted to other data modalities. \n",
        "\n",
        "### The MSWC FSCIL Neurobench Task:\n",
        "Aligning with a neuromorphic interest in temporal data modalities, this benchmark introduces a FSCIL task for streaming audio keyword classification using the large Multilingual Spoken Word Corpus (MSWC) dataset (https://mlcommons.org/datasets/multilingual-spoken-words/). The task is designed to be approached in two phases: pre-training and incremental learning:\n",
        "* First, for pre-training, a set of 100 words spanning 5 base languages (English, German, Catalan, French, Kinyarwanda) with 500 training samples each are made available to train an initial model. We provide here 2 pre-trained models, a convolutional and a recurrent spiking one, both trained with gradient descent on the train samples of the 100 base keywords.\n",
        "\n",
        "* Next, for incremental learning, the model undergoes 10 successive sessions to learn words from 10 new languages (Persian, Spanish, Russian, Welsh, Italian, Basque, Polish, Esparanto, Portuguese, Dutch) in a few-shot learning scenario. Each incremental session adds 10 words of the corresponding session language with only 5 training samples available per word. Here we give a tutorial for the prototypical network solution (https://arxiv.org/abs/1703.05175), as presented in the Neurobench paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing the Dataset\n",
        "\n",
        "The dataset is hosted on _Hugging Face_ at the following address: https://huggingface.co/datasets/NeuroBench/mswc_fscil_subset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code downloads the zipped file in a _data_ folder in the neurobench repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "relative_data_folder = '../../data'\n",
        "\n",
        "# URL of the .tar.gz file\n",
        "url = 'https://huggingface.co/datasets/NeuroBench/mswc_fscil_subset/resolve/main/mswc_fscil.tar.gz'\n",
        "\n",
        "# Download the file: It is less than 1 Gb, should be quick.\n",
        "response = requests.get(url)\n",
        "filename = os.path.join(relative_data_folder, url.split('/')[-1])\n",
        "\n",
        "with open(filename, 'wb') as file:\n",
        "    file.write(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we extract the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Open the tar file\n",
        "with tarfile.open(filename, 'r:gz') as tar:\n",
        "    # Get the total number of files within the tar archive (for the progress bar)\n",
        "    total_files = len(tar.getmembers())\n",
        "\n",
        "    # Set up the tqdm progress bar\n",
        "    with tqdm(total=total_files, unit='file', desc='Extracting files') as progress_bar:\n",
        "        for member in tar.getmembers():\n",
        "            # Extract each member\n",
        "            tar.extract(member, path=relative_data_folder)  # Replace with your desired path\n",
        "            # Update the progress bar\n",
        "            progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Benchmark Task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"/home3/p306982/Simulations/fscil/algorithms_benchmarks/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Import the modules required for running the benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "\n",
        "from neurobench.benchmarks import Benchmark\n",
        "from neurobench.datasets import MSWC\n",
        "from neurobench.datasets.MSWC_IncrementalLoader import IncrementalFewShot\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fix the default settings. Redefine them to your liking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ROOT = \"//scratch/p306982/data/fscil/FSCIL_subset/\" #\"neurobench/data/MSWC/\"\n",
        "\n",
        "NUM_WORKERS = 8\n",
        "BATCH_SIZE = 256\n",
        "NUM_SHOTS = 5 # How many shots to use for evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Select the desired device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if device == torch.device(\"cuda\"):\n",
        "    PIN_MEMORY = True\n",
        "else:\n",
        "    PIN_MEMORY = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, decide if you want to go through the tutorial with the spiking neural network (SNN) or the convolutional one (CNN). Precise this by setting `SPIKING` to True (SNN) or False (CNN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "SPIKING = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pre-trained model loading:\n",
        "\n",
        "We don't cover the pre-training here as it follows a standard gradient descent and can take quite some time.\n",
        "\n",
        "The pre-training step is nevertheless significant for the FSCIL performance. The models are pre-trained on the MSWC base training subset (in code: `MSWC(root=..., subset=\"base\", procedure=\"training\")`) which has 100 classes with 500 samples per class. The detailed pre-training procedure can be found in the _mswc_fscil.py_ code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neurobench.examples.mswc_fscil.M5 import M5\n",
        "from neurobench.models import TorchModel\n",
        "\n",
        "from neurobench.examples.mswc_fscil.sparchSNNs import SNN\n",
        "from neurobench.examples.mswc_fscil.sparchSNNs import RadLIFLayer\n",
        "\n",
        "MODEL_SAVE_DIR = \"./model_data/\"  #Folder where pre-trained models are stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the corresponding pre-trained model `mswc_rsnn_proto` (SNN) or `mswc_cnn_proto` (CNN) which are made available directly in the Neurobench github repo under the `examples/mswc_fscil/model_data/` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "if SPIKING:\n",
        "    model = SNN(\n",
        "        input_shape=(256, 201, 20),\n",
        "        neuron_type=\"RadLIF\",\n",
        "        layer_sizes=[1024, 1024, 200],\n",
        "        normalization=\"batchnorm\",\n",
        "        dropout=0.1,\n",
        "        bidirectional=False,\n",
        "        use_readout_layer=True,\n",
        "        ).to(device)\n",
        "    \n",
        "    state_dict = torch.load(os.path.join(MODEL_SAVE_DIR, \"mswc_rsnn_proto\"),\n",
        "                        map_location=device)\n",
        "    model.load_state_dict(state_dict)\n",
        "else:\n",
        "    model = M5(n_input=20, stride=2, n_channel=256, \n",
        "            n_output=200, input_kernel=4, pool_kernel=2, drop=True).to(device)\n",
        "\n",
        "    state_dict = torch.load(os.path.join(MODEL_SAVE_DIR, \"mswc_cnn_proto\"),\n",
        "                        map_location=device)\n",
        "    model.load_state_dict(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We display the model below. As you can see:\n",
        "\n",
        "- The CNN model follows the multilayer M5 architecture defined in https://arxiv.org/abs/1610.00087 with a tuned kernel size to match the employed pre-processing.\n",
        "- The SNN model consists of 2 recurrent spiking neuron layers and a linear readout layer, adapted from the sparchSNN library https://github.com/idiap/sparch. The spiking neurons are leaky integrate and fire neurons with an extra adaptive variable to mitigate the impact of average activity. All neuron parameters are trained heterogeneously during pre-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "M5(\n",
              "  (conv1): Conv1d(20, 256, kernel_size=(4,), stride=(2,))\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act1): ReLU()\n",
              "  (drop1): Dropout(p=0.2, inplace=False)\n",
              "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
              "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act2): ReLU()\n",
              "  (drop2): Dropout(p=0.2, inplace=False)\n",
              "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
              "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act3): ReLU()\n",
              "  (drop3): Dropout(p=0.2, inplace=False)\n",
              "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv4): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
              "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act4): ReLU()\n",
              "  (drop4): Dropout(p=0.2, inplace=False)\n",
              "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (output): Linear(in_features=512, out_features=200, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we convert the model to a NeuroBench TorchModel to allow for computational metric benchmarking. This creates hooks to the model activity functions. The neural network itself is now stored in `model.net`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neurobench.models import TorchModel\n",
        "\n",
        "model = TorchModel(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For manually defined activation modules, like the adapative LIF neuron used for the SNN model, we need to add this hook manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "if SPIKING: \n",
        "    model.add_activation_module(RadLIFLayer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pre-processing:\n",
        "\n",
        "For the proposed solution, we employ a state-of-the-art pre-processing, namely Mel Frequency Cepstral Coefficients (MFCC) to extract relevant frequency-based coefficients. We employ the torchaudio MFCC processor (https://pytorch.org/audio/main/generated/torchaudio.transforms.MFCC.html) and tune the hop length to fix the resolution to 200Hz and the number of mel coefficients to 20 for a reasonable number of input channels to the network.\n",
        "\n",
        "For the _spiking_ solution, a delta-encoding is added on top of MFCC to convert the signals to spikes. This is done with the Speech2Spike pipeline (https://dl.acm.org/doi/abs/10.1145/3584954.3584995) that has directly been integrated in Neurobench. We note that this adds a spiking threshold as an extra parameter. It was fixed to 1 following the Speech2Spikes initial observations here. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from neurobench.preprocessing import MFCCProcessor, S2SProcessor\n",
        "\n",
        "n_fft = 512\n",
        "win_length = None\n",
        "hop_length = 240\n",
        "n_mels = 20\n",
        "n_mfcc = 20\n",
        "\n",
        "if SPIKING:\n",
        "    encode = S2SProcessor(device, transpose=True)\n",
        "    config_change = {\"sample_rate\": 48000,\n",
        "                     \"hop_length\": 240}\n",
        "    encode.configure(threshold=1.0, **config_change)\n",
        "else:\n",
        "    encode = MFCCProcessor(\n",
        "        sample_rate=48000,\n",
        "        n_mfcc=n_mfcc,\n",
        "        melkwargs={\n",
        "            \"n_fft\": n_fft,\n",
        "            \"n_mels\": n_mels,\n",
        "            \"hop_length\": hop_length,\n",
        "            \"mel_scale\": \"htk\",\n",
        "            \"f_min\": 20,\n",
        "            \"f_max\": 4000,\n",
        "        },\n",
        "        device = device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preparation for Prototypical Continual Learning:\n",
        "\n",
        "Before we can start using the prototypical network approach for learning incremental classes, we need to align the pre-trained model with this approach. The prototypical network approach (https://arxiv.org/abs/1703.05175) indeed relies on implementing a clustering protocol, based on the pre-trained feature extractor, as a linear readout layer; but this requires all parameters of this readout layer to be defined accordingly. Thus we first redefine the readout layer for the 100 base classes following the prototypical network approach (such that they will align with the incremental classes prototypical readout parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First we load the base training dataset that is the data available to generate the prototypical representations for the base classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_train_set = MSWC(root=ROOT, subset=\"base\", procedure=\"training\")\n",
        "train_loader = DataLoader(base_train_set, batch_size=500, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prototypical readout parameters are defined based on the mean extracted feature $c_k$ from all training sample of the corresponding class $k$, which we get by passing all input samples through the backbone of the pre-trained network (all layers except the readout one). The prototypical weights and biases for class $k$ then are: $$W_k = 2c_k, \\ \\ b_k=c_kc_k^T$.$\n",
        "\n",
        "To do so, we first define a new readout layer that will replace the pre-trained one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set-up new proto readout layer\n",
        "if SPIKING:\n",
        "    output = model.net.snn[-1].W\n",
        "    proto_out = nn.Linear(output.weight.shape[1], 200, bias=True).to(device)\n",
        "    proto_out.weight.data = output.weight.data\n",
        "else:\n",
        "    output = model.net.output\n",
        "    proto_out = nn.Linear(512, 200, bias=True).to(device)\n",
        "    proto_out.weight.data = output.weight.data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we pass through each the base training classes, get all of the 500 associated sample feature, average them and define the weights and biases accordingly.\n",
        "\n",
        "Just note that for the _spiking_ solution, the features are summed over time and thus the bias is also divided by the number of total timesteps.\n",
        "\n",
        "_Note_: This procedure can take a bit of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:48<00:00,  2.06it/s]\n"
          ]
        }
      ],
      "source": [
        "# Compute prototype weights for base classes\n",
        "\n",
        "for data, target in tqdm(train_loader):\n",
        "    data, target = encode((data.to(device), target.to(device)))\n",
        "    data = data.squeeze()\n",
        "    class_id = target[0]\n",
        "\n",
        "    if SPIKING:\n",
        "        features = data\n",
        "        for layer in model.net.snn[:-1]:\n",
        "            features = layer(features)\n",
        "\n",
        "        mean = torch.sum(features, dim=[0,1])/500\n",
        "        proto_out.weight.data[class_id] = 2*mean\n",
        "        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())/features.shape[1]\n",
        "\n",
        "    else:\n",
        "        features = model.net(data, features_out=True)\n",
        "\n",
        "        mean = torch.sum(features, dim=0)/500\n",
        "        proto_out.weight.data[class_id] = 2*mean\n",
        "        proto_out.bias.data[class_id] = -torch.matmul(mean, mean.t())\n",
        "\n",
        "    del data\n",
        "    del features\n",
        "    del mean\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we replace the pre-trained readout layer by the newly defined prototypical one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Replace pre-trained readout with prototypical layer\n",
        "if SPIKING:\n",
        "    model.net.snn[-1].W = proto_out\n",
        "else:\n",
        "    model.net.output = proto_out\n",
        "\n",
        "del base_train_set\n",
        "del train_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we test the performance of the prototypical representations on the base test set using a Neurobench Benchmark:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "M5(\n",
              "  (conv1): Conv1d(20, 256, kernel_size=(4,), stride=(2,))\n",
              "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act1): ReLU()\n",
              "  (drop1): Dropout(p=0.2, inplace=False)\n",
              "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,))\n",
              "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act2): ReLU()\n",
              "  (drop2): Dropout(p=0.2, inplace=False)\n",
              "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv3): Conv1d(256, 512, kernel_size=(3,), stride=(1,))\n",
              "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act3): ReLU()\n",
              "  (drop3): Dropout(p=0.2, inplace=False)\n",
              "  (pool3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv4): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n",
              "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act4): ReLU()\n",
              "  (drop4): Dropout(p=0.2, inplace=False)\n",
              "  (pool4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (output): Linear(in_features=512, out_features=200, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Copy model for evaluation\n",
        "eval_model = copy.deepcopy(model)\n",
        "\n",
        "# Get base test set for evaluation\n",
        "base_test_set = MSWC(root=ROOT, subset=\"base\", procedure=\"testing\")\n",
        "test_loader = DataLoader(base_test_set, batch_size=256, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
        "\n",
        "# Put the model in evaluation mode\n",
        "eval_model.net.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As Neurobench Benchmarks encapsulate the whole testing, it requires some pre and post-processors to manipulate data before and aftera network pass. We thus define the following utility functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "squeeze = lambda x: (x[0].squeeze(), x[1])\n",
        "out2pred = lambda x: torch.argmax(x, dim=-1)\n",
        "to_device = lambda x: (x[0].to(device), x[1].to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also define a mask function for this evaluation as the network is directly defined with 200 output neurons but we are for now evaluating the performance solely on the 100 base classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define specific post-processing with masking on the base classes\n",
        "mask = torch.full((200,), float('inf')).to(device)\n",
        "mask[torch.arange(0,100, dtype=int)] = 0\n",
        "out_mask = lambda x: x - mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can define the Benchmark object with the desired metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics\n",
        "static_metrics = [\"model_size\", \"connection_sparsity\"]\n",
        "workload_metrics = [\"classification_accuracy\", \"activation_sparsity\", \"synaptic_operations\"]\n",
        "\n",
        "# Define benchmark object\n",
        "benchmark_all_test = Benchmark(eval_model, metric_list=[static_metrics, workload_metrics], \n",
        "                               dataloader=test_loader, \n",
        "                               preprocessors=[to_device, encode, squeeze], postprocessors=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now run the Benchmark on the base test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_train_results = benchmark_all_test.run(postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "\n",
        "print(\"Base results:\", pre_train_results)\n",
        "\n",
        "print(f\"The base accuracy is {pre_train_results['classification_accuracy']*100}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is the performance of session 0.\n",
        "\n",
        "Note that the obtained accuracy, after conversion to prototypes, is below the original performance of the pre-trained model. This is a price to pay to allow for the prototypical network to work effectively in the incremental sessions. This could nevertheless still be improved upon, especially for the _spiking_ solution, where the conversion accuracy drop is significant (from 93% to 84%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Incremental Learning:\n",
        "\n",
        "We can now pursue with the few-shot incremental sessions. New sessions are learned following the prototypical network approach on the corresponding session classes and with the limited number of samples available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first initialize the FSCIL dataloader. It will generate 10 sessions from a random ordering of the 10 incremental languages. Each session consists of\n",
        "- 1 `support` _list_ of `NUM_SHOTS` shots, each shot being a tupple of tensors `(X_shot, y_shot)` with 1 sample for each of the 10 session classes.  \n",
        "- 1 `query` _dataset_ with all the current and prior incremental session classes and `query_shots` samples per class.\n",
        "- 1 `query_classes` list that contains each unique incremental class index following their order of appearance.\n",
        "\n",
        "Note that the `support_query_split` is here to define a pre-sampling split between samples available for support and for query in this order. In the proposed set-up, the few-shot dataloader thus fixes the 100 query samples per class from the start and samples 5 shots out of a 100 samples for each incremental class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IncrementalFewShot Dataloader used in incremental mode to generate class-incremental sessions\n",
        "few_shot_dataloader = IncrementalFewShot(k_shot=NUM_SHOTS, \n",
        "                            root = ROOT,\n",
        "                            query_shots=100,\n",
        "                            support_query_split=(100,100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then run 1 incremental session learning as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "support, query, query_classes = next(iter(few_shot_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The support data - which is generated in a shot-by-shot way for universality to different methods - is here concatenated to gather all training samples per class for the prototypical approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = None\n",
        "\n",
        "for X_shot, y_shot in support:\n",
        "    if data is None:\n",
        "        data = X_shot\n",
        "        target = y_shot\n",
        "    else:\n",
        "        data = torch.cat((data,X_shot), 0)\n",
        "        target = torch.cat((target,y_shot), 0)\n",
        "\n",
        "data, target = encode((data.to(device), target.to(device)))\n",
        "data = data.squeeze()\n",
        "\n",
        "new_classes = y_shot.tolist()\n",
        "Nways = len(y_shot) #Number of ways, should always be 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then apply the prototypical network approach on the corresponding incremental classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "if SPIKING:\n",
        "    features = eval_model.net.snn[0](data)\n",
        "    features = eval_model.net.snn[1](features)\n",
        "\n",
        "    for index, class_id in enumerate(new_classes):\n",
        "        mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=[0,1])/NUM_SHOTS\n",
        "        eval_model.net.snn[-1].W.weight.data[class_id] = 2*mean\n",
        "        eval_model.net.snn[-1].W.bias.data[class_id] = -torch.matmul(mean, mean.t())/(features.shape[1])\n",
        "else:\n",
        "    features = eval_model.net(data, features_out=True)\n",
        "\n",
        "    for index, class_id in enumerate(new_classes):\n",
        "        mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=0)/NUM_SHOTS\n",
        "        eval_model.net.output.weight.data[class_id] = 2*mean\n",
        "        eval_model.net.output.bias.data[class_id] = -torch.matmul(mean, mean.t())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we evaluate the performance after 1 FSCIL session. The default FSCIL benchmarking evaluates accuracy on all classes seen so far, including the base classes used for pre-training. To this, we add an evaluation of the performance solely on the incremental few-shot classes, corresponding to only the `query` dataset.\n",
        "\n",
        "Note that the dataloaders used for the benchmarking are actually redefined when running the Benchmark object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:02<00:00,  1.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 85.1 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 43/43 [00:11<00:00,  3.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 93.7181817921725 %\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Define benchmark object for incremental classes\n",
        "benchmark_new_classes = Benchmark(eval_model, metric_list=[[],[\"classification_accuracy\"]], \n",
        "                                  dataloader=None,\n",
        "                                  preprocessors=[to_device, encode, squeeze], postprocessors=[])    \n",
        "\n",
        "### Testing phase ###\n",
        "eval_model.net.eval()\n",
        "\n",
        "# Define session dataloaders for query and query + base_test samples\n",
        "query_loader = DataLoader(query, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "full_session_test_set = ConcatDataset([base_test_set, query])\n",
        "full_session_test_loader = DataLoader(full_session_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "# Create a mask function to only consider accuracy on classes presented so far\n",
        "session_classes = torch.cat((torch.arange(0,100, dtype=int), torch.IntTensor(query_classes))) \n",
        "mask = torch.full((200,), float('inf')).to(device)\n",
        "mask[session_classes] = 0\n",
        "out_mask = lambda x: x - mask\n",
        "\n",
        "\n",
        "# Run benchmark on query classes only\n",
        "query_results = benchmark_new_classes.run(dataloader = query_loader, \n",
        "                                          postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "print(f\"Accuracy on new classes: {query_results['classification_accuracy']*100} %\")\n",
        "\n",
        "# Run benchmark to evaluate accuracy of this specific session\n",
        "session_results = benchmark_all_test.run(dataloader = full_session_test_loader, \n",
        "                                         postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "print(f\"Session accuracy: {session_results['classification_accuracy']*100} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can run the full FSCIL setup following the set-up presented above.\n",
        "\n",
        "_Note_: This can take a bit of time as FSCIL requires for increasingly heavy datasets to be loaded in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session: 1\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:02<00:00,  1.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 77.49999947547913 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 43/43 [00:11<00:00,  3.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 92.86363624225963 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0001, 'classification_accuracy': 0.9286363624225963, 'activation_sparsity': 0.7838201819435159, 'synaptic_operations': {'Effective_MACs': 7821753.346363637, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 2\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:02<00:00,  3.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 76.55000061988831 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 47/47 [00:11<00:00,  4.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 91.22500012715656 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0001, 'classification_accuracy': 0.9122500012715656, 'activation_sparsity': 0.7830629994510823, 'synaptic_operations': {'Effective_MACs': 7838990.875666667, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 3\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12/12 [00:04<00:00,  2.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 76.06666660308838 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 51/51 [00:12<00:00,  4.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 89.89230761894815 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0001, 'classification_accuracy': 0.8989230761894815, 'activation_sparsity': 0.7824786216770362, 'synaptic_operations': {'Effective_MACs': 7855375.257076923, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 4\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:04<00:00,  3.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 72.0 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 55/55 [00:12<00:00,  4.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 87.61428576878143 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0001, 'classification_accuracy': 0.8761428576878143, 'activation_sparsity': 0.781472531786152, 'synaptic_operations': {'Effective_MACs': 7884946.772571429, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 5\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:06<00:00,  3.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 73.51999997138978 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 59/59 [00:13<00:00,  4.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 86.43999997774759 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0003, 'classification_accuracy': 0.864399999777476, 'activation_sparsity': 0.7827006714665032, 'synaptic_operations': {'Effective_MACs': 7848042.198133334, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 6\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24/24 [00:06<00:00,  3.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 74.58333336512248 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 63/63 [00:14<00:00,  4.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 85.97500000000001 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0003, 'classification_accuracy': 0.8597500000000001, 'activation_sparsity': 0.7829007939357385, 'synaptic_operations': {'Effective_MACs': 7845437.624125, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 7\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28/28 [00:07<00:00,  3.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 74.94285717010497 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 67/67 [00:15<00:00,  4.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 85.3705882577335 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0003, 'classification_accuracy': 0.853705882577335, 'activation_sparsity': 0.7829895382785469, 'synaptic_operations': {'Effective_MACs': 7840129.522588235, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 8\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:07<00:00,  4.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 75.49999999999999 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 71/71 [00:15<00:00,  4.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 84.96666668256124 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0004, 'classification_accuracy': 0.8496666668256124, 'activation_sparsity': 0.7827420523982161, 'synaptic_operations': {'Effective_MACs': 7846225.098611111, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 9\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 36/36 [00:08<00:00,  4.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 76.45555556615193 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 75/75 [00:16<00:00,  4.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 84.82631579951233 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0004, 'classification_accuracy': 0.8482631579951233, 'activation_sparsity': 0.7829535261061661, 'synaptic_operations': {'Effective_MACs': 7840869.03668421, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n",
            "Session: 10\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 40/40 [00:09<00:00,  4.25it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on new classes: 76.13000000000001 %\n",
            "Running benchmark\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 79/79 [00:17<00:00,  4.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session accuracy: 84.19999999999999 %\n",
            "Session results: {'model_size': 6028096, 'connection_sparsity': 0.0005, 'classification_accuracy': 0.8419999999999999, 'activation_sparsity': 0.7828598728553922, 'synaptic_operations': {'Effective_MACs': 7845202.8584, 'Effective_ACs': 0.0, 'Dense': 25919488.0}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Iteration over incremental sessions\n",
        "for session, (support, query, query_classes) in enumerate(few_shot_dataloader):\n",
        "    print(f\"Session: {session+1}\")\n",
        "\n",
        "    ### Computing new Prototypical Weights ###\n",
        "    data = None\n",
        "    \n",
        "    for X_shot, y_shot in support:\n",
        "        if data is None:\n",
        "            data = X_shot\n",
        "            target = y_shot\n",
        "        else:\n",
        "            data = torch.cat((data,X_shot), 0)\n",
        "            target = torch.cat((target,y_shot), 0)\n",
        "\n",
        "    data, target = encode((data.to(device), target.to(device)))\n",
        "    data = data.squeeze()\n",
        "\n",
        "    new_classes = y_shot.tolist()\n",
        "    Nways = len(y_shot) #Number of ways, should always be 10\n",
        "\n",
        "    if SPIKING:\n",
        "        features = eval_model.net.snn[0](data)\n",
        "        features = eval_model.net.snn[1](features)\n",
        "\n",
        "        for index, class_id in enumerate(new_classes):\n",
        "            mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=[0,1])/NUM_SHOTS\n",
        "            eval_model.net.snn[-1].W.weight.data[class_id] = 2*mean\n",
        "            eval_model.net.snn[-1].W.bias.data[class_id] = -torch.matmul(mean, mean.t())/(features.shape[1])\n",
        "    else:\n",
        "        features = eval_model.net(data, features_out=True)\n",
        "\n",
        "        for index, class_id in enumerate(new_classes):\n",
        "            mean = torch.sum(features[[i*Nways+index for i in range(NUM_SHOTS)]], dim=0)/NUM_SHOTS\n",
        "            eval_model.net.output.weight.data[class_id] = 2*mean\n",
        "            eval_model.net.output.bias.data[class_id] = -torch.matmul(mean, mean.t())\n",
        "\n",
        "    ### Testing phase ###\n",
        "    eval_model.net.eval()\n",
        "\n",
        "    # Define session dataloaders for query and query + base_test samples\n",
        "    query_loader = DataLoader(query, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "    \n",
        "    full_session_test_set = ConcatDataset([base_test_set, query])\n",
        "    full_session_test_loader = DataLoader(full_session_test_set, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # Create a mask function to only consider accuracy on classes presented so far\n",
        "    session_classes = torch.cat((torch.arange(0,100, dtype=int), torch.IntTensor(query_classes))) \n",
        "    mask = torch.full((200,), float('inf')).to(device)\n",
        "    mask[session_classes] = 0\n",
        "    out_mask = lambda x: x - mask\n",
        "\n",
        "    # Run benchmark on query classes only\n",
        "    query_results = benchmark_new_classes.run(dataloader = query_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "    print(f\"Accuracy on new classes: {query_results['classification_accuracy']*100} %\")\n",
        "\n",
        "    # Run benchmark to evaluate accuracy of this specific session\n",
        "    session_results = benchmark_all_test.run(dataloader = full_session_test_loader, postprocessors=[out_mask, F.softmax, out2pred, torch.squeeze])\n",
        "    print(f\"Session accuracy: {session_results['classification_accuracy']*100} %\")\n",
        "    print(\"Session results:\", session_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You should obtain a performance within the bounds presented in the results plot below. The shaded area represents $5^{th}$ and $95^{th}$ percentile on 100 runs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![title](img/FSCIL_proto_results.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
