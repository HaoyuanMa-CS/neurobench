{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGm4fad3M-Sr"
      },
      "source": [
        "# DVS Gesture Benchmark Tutorial\n",
        "\n",
        "This tutorial aims to provide an insight on how the NeuroBench framework is organized and how you can use it to benchmark your own models!\n",
        "\n",
        "## About DVS Gesture:\n",
        "Mid-air gestures represent a natural modality of communication that holds benefits in a range of human-computer interaction applications owing to its touchless and efficient nature. Gesture recognition systems find utility in edge scenarios such as car infotainment systems, high-traffic public areas, or as alternative interaction modes for individuals with speech or hearing impairments.\n",
        "\n",
        "### Dataset:\n",
        "The IBM Dynamic Vision Sensor (DVS) Gesture dataset is composed of recordings of 29 distinct individuals executing 10 different types of gestures, including but not limited to clapping, waving, etc. Additionally, an 11th gesture class is\n",
        "included that comprises gestures that cannot be categorized within the first 10 classes. The gestures are recorded under four distinct lighting conditions, and each gesture is associated with a label that indicates the corresponding lighting condition under which it was performed.\n",
        "\n",
        "### Benchmark Task:\n",
        "The task is a classification task. The benchmark task is to use samples from the 23 initial subjects as training and generalize to samples from the remaining 6 subjects.\n",
        "\n",
        "First we will import the relevant libraries. These include the datasets, preprocessors and accumulators. To ensure your model to be compatible with the NeuroBench framework, we will import the wrapper for snnTorch models. This wrapper will not change your model. Finally, we import the Benchmark class, which will run the benchmark and calculate your metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqtM6XbMM_hO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import snntorch as snn\n",
        "\n",
        "from torch import nn\n",
        "from snntorch import surrogate\n",
        "\n",
        "from neurobench.datasets import DVSGesture\n",
        "from neurobench.models import SNNTorchModel\n",
        "from neurobench.benchmarks import Benchmark\n",
        "from neurobench.accumulators.accumulator import aggregate,choose_max_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7HMjVPX7LZh"
      },
      "source": [
        "For this tutorial, we will make use of the example architecture that is included in the NeuroBench framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0yYDNRZ7UxY"
      },
      "outputs": [],
      "source": [
        "# this is the network we will be using in this tutorial\n",
        "from neurobench.examples.dvs_gesture.CSNN import Conv_SNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNIgTfvuOMe-"
      },
      "source": [
        "To get started, we will load our desired dataset in a dataloader. Note that upon initializing the dataset, the preprocessing can be applied to the dataset, avoiding the need for further preprocessing in the benchmark itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chZeyUTAOQ6B"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_set = DVSGesture(\"data/dvs_gesture/\", split=\"testing\", preprocessing=\"stack\")\n",
        "test_set_loader = DataLoader(test_set, batch_size=16, shuffle=True,drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTB808RoNXqL"
      },
      "source": [
        "Next, load our model and wrap it in the corresponding NeuroBench wrapper. At the time of writing this tutorial, (V1.0) snnTorch is the only supported framework, therefore, we will wrap our snnTorch model in the SNNTorchModel() wrapper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4jOfnt6OeIH"
      },
      "outputs": [],
      "source": [
        "net = Conv_SNN()\n",
        "net.load_state_dict(torch.load('neurobench/examples/dvs_gesture/model_data/DVS_SNN_untrained.pth'))\n",
        "\n",
        "# wrap in SNNTorchModel wrapper\n",
        "model = SNNTorchModel(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfRfdvXvOqRP"
      },
      "source": [
        "Specify the preprocessor and postprocessor want to use. These will be applied to your data before feeding into the model, and to the output spikes respectively.\n",
        "Available preprocessors and postprocessors can be found in neurobench/preprocessors and neurobench/accumulators respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GHY8vTROwzP"
      },
      "outputs": [],
      "source": [
        "# the dataset is already in the correct format, no preprocessing is needed\n",
        "preprocessing = []\n",
        "\n",
        "# postprocessors\n",
        "postprocessors = [choose_max_count]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9doNsI0O0Jl"
      },
      "source": [
        "Next specify the metrics which you want to calculate. The available metrics (V1.0 release) are:\n",
        "\n",
        "static_metrics:\n",
        "*   model_size\n",
        "*   connection_sparsity\n",
        "*   frequency\n",
        "\n",
        "\n",
        "workload_metrics\n",
        "*   activation_sparsity\n",
        "*   multiply_accumulates\n",
        "*   classification_accuracy\n",
        "\n",
        "More explanation on the metrics can be found on https://neurobench.ai/."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDUczVTkPOsQ"
      },
      "outputs": [],
      "source": [
        "static_metrics = [\"model_size\"]\n",
        "workload_metrics = [\"synaptic_operations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXQYfiJpPTZb"
      },
      "source": [
        "Next, we instanciate the benchmark. We have to specify the model, the dataloader, the preprocessors, the postprocessor and the list of the static and data metrics which we want to measure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0_N96ADPeO5"
      },
      "outputs": [],
      "source": [
        "benchmark = Benchmark(model, test_set_loader, preprocessors, postprocessors, [static_metrics, workload_metrics])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ytLJ-dUPp0b"
      },
      "source": [
        "Now, let's run the benchmark and print our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ldww7kiYPsU2"
      },
      "outputs": [],
      "source": [
        "results = benchmark.run()\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpreting the results can now lead to a better understanding of your model. As can be seen in this tutorial, the synaptic operations of our CSNN include Multiply-Accumulates as well. This might seem counterintuitive initially, but when looking at the layers that actually cause these, it is found that is is caused by the first layer. Even though we asked for stack preprocessing which preserves the spiking nature of the DVS Gesture dataset. Stack preprocessing bins positive and negative events in a certain time window, equalling the pixels which received positive events to 1 in the positive channel and similarly for the negative channel. However, before passing these events to our first convolutional layer, we apply a pooling layer which creates values which are not zero or one, leading to the need of MACs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
